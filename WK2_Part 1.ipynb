{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 619 Deep Learning\n",
    "# Week 2: Perform Regressions Using Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "After you complete this module, students will be able to:\n",
    "\n",
    "+ Perform feature engineering and convert categorical variables to numerical variables\n",
    "+ Check the model fitting by plotting training loss and validation loss\n",
    "+ Address overfitting by using dropout in the networks\n",
    "+ Perform hyperparameter tuning for neuron networks\n",
    "\n",
    "\n",
    "Linear regression is a classical method in machine learning and deep learning. First, we cover data cleaning and feature engineering in neuron networks.  Next, we will cover the underfitting and overfitting of deep learning models. Third, we will show how to address overfitting by using dropout in the neuron networks. Finally, we will learn how to perform hyperparameter tuning to improve model performance. \n",
    "\n",
    "**Readings**\n",
    "\n",
    "+ Basic regression: Predict fuel efficiency (https://www.tensorflow.org/tutorials/keras/regression)\n",
    "+ Dropout: A Simple Way to Prevent Neural Networks from Overfitting (https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "+ Introduction to the Keras Tuner (https://www.tensorflow.org/tutorials/keras/keras_tuner)\n",
    "+ Linear Neural Networks (https://d2l.ai/chapter_linear-networks/index.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project of Seoul Bike Sharing Demand Using TensorFlow\n",
    "\n",
    "Let’s look at a real-world project of regression using TensorFlow. By performing this regression model, we will learn more new topics in deep learning.\n",
    "\n",
    "Our dataset is taken from UCI Machine Learning Repository (at https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand).\n",
    "\n",
    "This dataset containing in SeoulBikeData.csv has the following features/predictors:\n",
    "\n",
    "+ **Date** : year-month-day\n",
    "+ **Hour** - Hour of he day\n",
    "+ **Temperature**-Temperature in Celsius\n",
    "+ **Humidity** - %\n",
    "+ **Windspeed** - m/s\n",
    "+ **Visibility** - 10m\n",
    "+ **Dew point temperature** - Celsius\n",
    "+ **Solar radiation** - MJ/m2\n",
    "+ **Rainfall** - mm\n",
    "+ **Snowfall** - cm\n",
    "+ **Seasons** - Winter, Spring, Summer, Autumn\n",
    "+ **Holiday** - Holiday/No holiday\n",
    "+ **Functional Day** - NoFunc(Non Functional Hours), Fun(Functional hours)\n",
    "\n",
    "The corresponding label/target is:\n",
    "\n",
    "+ **Rented Bike count** - Count of bikes rented at each hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean the Data\n",
    "\n",
    "First, we load the data into memory using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv('SeoulBikeData.csv', sep = ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But Python produces the following error message.\n",
    "\n",
    "<font color='red'>UnicodeDecodeError </font>: 'utf-8' codec can't decode byte 0xb0 in position 12: invalid start byte\n",
    "\n",
    "Pandas use a default encoding of 'utf-8' code. But it cannot recognize the data format in the 'utf-8' code. To fix this bug, we can try different encoding formats. Python library supports about one hundred different encodings. Please see the complete list of encoding at https://docs.python.org/3/library/codecs.html#standard-encodings.\n",
    "\n",
    "Unfortunately, there is no easy way to determine the encoding format. We probably need to guess it. We can always try encoding = \"ISO-8859-1\", which supports the western European language. If it doesn't work, we should try another encoding from the supported list. (see: https://docs.python.org/3/library/codecs.html#standard-encodings)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(°C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(°C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Functioning Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>37</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>36</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2000</td>\n",
       "      <td>-18.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Rented Bike Count  Hour  Temperature(°C)  Humidity(%)  \\\n",
       "0  1/12/2017                254     0             -5.2           37   \n",
       "1  1/12/2017                204     1             -5.5           38   \n",
       "2  1/12/2017                173     2             -6.0           39   \n",
       "3  1/12/2017                107     3             -6.2           40   \n",
       "4  1/12/2017                 78     4             -6.0           36   \n",
       "\n",
       "   Wind speed (m/s)  Visibility (10m)  Dew point temperature(°C)  \\\n",
       "0               2.2              2000                      -17.6   \n",
       "1               0.8              2000                      -17.6   \n",
       "2               1.0              2000                      -17.7   \n",
       "3               0.9              2000                      -17.6   \n",
       "4               2.3              2000                      -18.6   \n",
       "\n",
       "   Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm) Seasons     Holiday  \\\n",
       "0                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "1                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "2                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "3                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "4                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "\n",
       "  Functioning Day  \n",
       "0             Yes  \n",
       "1             Yes  \n",
       "2             Yes  \n",
       "3             Yes  \n",
       "4             Yes  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('SeoulBikeData.csv', encoding = \"ISO-8859-1\",sep = ',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to check the data type for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                          object\n",
       "Rented Bike Count              int64\n",
       "Hour                           int64\n",
       "Temperature(°C)              float64\n",
       "Humidity(%)                    int64\n",
       "Wind speed (m/s)             float64\n",
       "Visibility (10m)               int64\n",
       "Dew point temperature(°C)    float64\n",
       "Solar Radiation (MJ/m2)      float64\n",
       "Rainfall(mm)                 float64\n",
       "Snowfall (cm)                float64\n",
       "Seasons                       object\n",
       "Holiday                       object\n",
       "Functioning Day               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check the missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                         0\n",
       "Rented Bike Count            0\n",
       "Hour                         0\n",
       "Temperature(°C)              0\n",
       "Humidity(%)                  0\n",
       "Wind speed (m/s)             0\n",
       "Visibility (10m)             0\n",
       "Dew point temperature(°C)    0\n",
       "Solar Radiation (MJ/m2)      0\n",
       "Rainfall(mm)                 0\n",
       "Snowfall (cm)                0\n",
       "Seasons                      0\n",
       "Holiday                      0\n",
       "Functioning Day              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing values in all columns. But there are several features are categorical variables. Let's look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seasons            4\n",
       "Holiday            2\n",
       "Functioning Day    2\n",
       "Name: unique, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catFeatures = ['Seasons', 'Holiday', 'Functioning Day']\n",
    "df[catFeatures].describe(include='all').loc['unique', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Features to Numerical Features \n",
    "\n",
    "We notice that the following features/columns are not numerical variables that include float 64 and int64.\n",
    "+ Date\n",
    "+ Seasons\n",
    "+ Holiday\n",
    "+ Functioning Day \n",
    "\n",
    "They are all object data types. Deep learning can only handle numerical data. We need to convert them to the numerical data such as int64 or float64.\n",
    "\n",
    "Seasons, Holiday and Functioning Day are categorical variables that have only finite many cases. Let's convert them to numerical variables. We use get_dummies function by specifying drop_first = True to **reduce the redundant feature**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize  the unique values for all the categorical features/variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seasons_Spring</th>\n",
       "      <th>Seasons_Summer</th>\n",
       "      <th>Seasons_Winter</th>\n",
       "      <th>Holiday_No Holiday</th>\n",
       "      <th>Functioning Day_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Seasons_Spring  Seasons_Summer  Seasons_Winter  Holiday_No Holiday  \\\n",
       "0           False           False            True                True   \n",
       "1           False           False            True                True   \n",
       "2           False           False            True                True   \n",
       "3           False           False            True                True   \n",
       "4           False           False            True                True   \n",
       "\n",
       "   Functioning Day_Yes  \n",
       "0                 True  \n",
       "1                 True  \n",
       "2                 True  \n",
       "3                 True  \n",
       "4                 True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catFeatures = ['Seasons', 'Holiday','Functioning Day']\n",
    "factors = pd.get_dummies(df[catFeatures],drop_first=True)\n",
    "factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop the original categorical variables, then concatenate the numerical features and dummy variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(°C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(°C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "      <th>Seasons_Spring</th>\n",
       "      <th>Seasons_Summer</th>\n",
       "      <th>Seasons_Winter</th>\n",
       "      <th>Holiday_No Holiday</th>\n",
       "      <th>Functioning Day_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>37</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>36</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2000</td>\n",
       "      <td>-18.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Rented Bike Count  Hour  Temperature(°C)  Humidity(%)  \\\n",
       "0  1/12/2017                254     0             -5.2           37   \n",
       "1  1/12/2017                204     1             -5.5           38   \n",
       "2  1/12/2017                173     2             -6.0           39   \n",
       "3  1/12/2017                107     3             -6.2           40   \n",
       "4  1/12/2017                 78     4             -6.0           36   \n",
       "\n",
       "   Wind speed (m/s)  Visibility (10m)  Dew point temperature(°C)  \\\n",
       "0               2.2              2000                      -17.6   \n",
       "1               0.8              2000                      -17.6   \n",
       "2               1.0              2000                      -17.7   \n",
       "3               0.9              2000                      -17.6   \n",
       "4               2.3              2000                      -18.6   \n",
       "\n",
       "   Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  Seasons_Spring  \\\n",
       "0                      0.0           0.0            0.0           False   \n",
       "1                      0.0           0.0            0.0           False   \n",
       "2                      0.0           0.0            0.0           False   \n",
       "3                      0.0           0.0            0.0           False   \n",
       "4                      0.0           0.0            0.0           False   \n",
       "\n",
       "   Seasons_Summer  Seasons_Winter  Holiday_No Holiday  Functioning Day_Yes  \n",
       "0           False            True                True                 True  \n",
       "1           False            True                True                 True  \n",
       "2           False            True                True                 True  \n",
       "3           False            True                True                 True  \n",
       "4           False            True                True                 True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(catFeatures,axis=1)\n",
    "df = pd.concat([df,factors],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Feature Engineering on Date Feature\n",
    "\n",
    "The Date column is not a categorical variable. It is a DateTime format. Therefore, we need to convert it to datetime format in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whitl\\AppData\\Local\\Temp\\ipykernel_21036\\1317676032.py:1: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] =  pd.to_datetime(df['Date'], infer_datetime_format=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"13/12/2017\" doesn't match format \"%m/%d/%Y\", at position 12. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], infer_datetime_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\OneDriveDesktopNew_Folder\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1108\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1106\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1108\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1110\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32m~\\OneDriveDesktopNew_Folder\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:254\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    252\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 254\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m convert_listlike(unique_dates, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\OneDriveDesktopNew_Folder\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:488\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m    490\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[0;32m    491\u001b[0m     arg,\n\u001b[0;32m    492\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDriveDesktopNew_Folder\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:519\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    509\u001b[0m     arg,\n\u001b[0;32m    510\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001b[1;32mstrptime.pyx:534\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:355\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"13/12/2017\" doesn't match format \"%m/%d/%Y\", at position 12. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "\n",
    "df['Date'] =  pd.to_datetime(df['Date'], infer_datetime_format=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                          object\n",
       "Rented Bike Count              int64\n",
       "Hour                           int64\n",
       "Temperature(°C)              float64\n",
       "Humidity(%)                    int64\n",
       "Wind speed (m/s)             float64\n",
       "Visibility (10m)               int64\n",
       "Dew point temperature(°C)    float64\n",
       "Solar Radiation (MJ/m2)      float64\n",
       "Rainfall(mm)                 float64\n",
       "Snowfall (cm)                float64\n",
       "Seasons_Spring                  bool\n",
       "Seasons_Summer                  bool\n",
       "Seasons_Winter                  bool\n",
       "Holiday_No Holiday              bool\n",
       "Functioning Day_Yes             bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may exist some seasonality in the data. Therefore, we want to extract the year, the month from the datetime feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#extract the year\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#extract the month\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n",
      "File \u001b[1;32m~\\OneDriveDesktopNew_Folder\\Lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[1;32m~\\OneDriveDesktopNew_Folder\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\OneDriveDesktopNew_Folder\\Lib\\site-packages\\pandas\\core\\indexes\\accessors.py:608\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "#extract the year\n",
    "df['year'] = df['Date'].dt.year\n",
    "#extract the month\n",
    "df['month'] = df['Date'].dt.month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the bike rental may have some differences between weekdays and weekends. Let's extract the day of the week from the Date feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['dayofweek']=df['Date'].dt.dayofweek\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to drop the Date column since we extract its year, month and day in terms of weekend or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Date',axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerically Summarize the Data\n",
    "\n",
    "Let's numerically summarize continuous variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#specify the continuous features\n",
    "numerics =['Rented Bike Count', 'Hour', 'Temperature(°C)', 'Humidity(%)',\n",
    "       'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)',\n",
    "       'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)', 'dayofweek'\n",
    "       ]\n",
    "#summarize it\n",
    "np.round(df[numerics].describe(), decimals=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correlation between all these numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(df[numerics].corr(), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphically Summarize the Data\n",
    "Let's summarize the numerical features graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#Pair plot continuous features\n",
    "#Disable all warnings in Juyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sns.pairplot(df[['Rented Bike Count', 'Temperature(°C)', 'Humidity(%)',\n",
    "       'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)',\n",
    "       'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's box plot count against the hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.boxplot(x='Hour',y='Rented Bike Count',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's box plot count against the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='month',y='Rented Bike Count',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's box plot count against the day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='dayofweek',y='Rented Bike Count',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data Into Training and Test Data Set\n",
    "\n",
    "We need first to combine all features into $X$ and select the label column as $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain features and label\n",
    "X = df.drop('Rented Bike Count',axis=1)\n",
    "y = df['Rented Bike Count']\n",
    "\n",
    "#Split the data into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization for the Neural Networks\n",
    "\n",
    "It is a good practice to normalize all features in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#Fit and transform the training data\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "#Only transform the test data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "\n",
    "Let's use the sequential model for this linear regression. The sequential model is a simple deep learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the neural network architecture.\n",
    "Let's build a neuron network with the following layers:\n",
    "+ Input layer with 100 neurons\n",
    "+ First hidden layer with 50 neurons\n",
    "+ Second hidden layer with 25 neurons\n",
    "+ Output layer with one neuron since it is a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# Input layer has 100 neurons\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "# First hidden layer with 50 neurons\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "# Second hidden layer with 50 neurons\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "# Output layer has one and only one neuron\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the Model by Seting Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to configure the training model by setting the optimization algorithms to find the optimal weights and specify the loss/error function.\n",
    "\n",
    "+ There are many optimization algorithms available in TensorFlow. We use one of the popular gradient-based algorithms Adam method.\n",
    "+ **For the regression problem, the loss/error function is the mean squared error**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure the model by choosing optimizer and loss function\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=ja\n",
    "\n",
    "\n",
    "We train the model by calling the fit method and specifying the following parameters;\n",
    "\n",
    "+ x is the features\n",
    "+ y is the label/target\n",
    "+ batch_size is the number of samples per gradient update. If unspecified, batch_size will default to 32. \n",
    "+ epochs is the number of epochs to train the model. An epoch is an iteration over **the entire x and y data provided**. \n",
    "+ validation_data is the data on which to evaluate the loss and any model metrics at the end of each epoch. This is the validation/test error since **the model doesn't see this data when it is trained based on x and y**.\n",
    "+ verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment).\n",
    "\n",
    "We run the magic command of **%% time** in the cell. It must be the first line of the code in the cell. It will print out the wall time for this cell. \n",
    "\n",
    "We save the model results in history. Therefore, we set verbose = 0 to avoid the printing out training info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Fix the seed \n",
    "tf.random.set_seed(1)\n",
    "#Fit the model and save the results in history\n",
    "history = model.fit(x=X_train,y=y_train,batch_size=64,epochs=100,\n",
    "          validation_data=(X_test,y_test), verbose=0\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the train and validation loss to a df\n",
    "trainhist = pd.DataFrame(history.history)\n",
    "#Add the epoch index\n",
    "trainhist['epoch'] = history.epoch\n",
    "#Look at the latest performance\n",
    "trainhist.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the train and validation loss/error on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plot train loss\n",
    "sns.lineplot(x='epoch', y ='loss', data =trainhist)\n",
    "#Plot validation loss\n",
    "sns.lineplot(x='epoch', y ='val_loss', data =trainhist)\n",
    "#Add legends\n",
    "plt.legend(labels=['train_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfit and Overfit of the Deep Learning Algorithms\n",
    "\n",
    "The deep learning model typically has many weights (parameters) to estimate. For example, a CNN model to cover in the later week may have several millions of parameters to estimate. Find the optimal weights in high-dimensional space is very challenging. The algorithm may trap in local minimum instead of the global minimum. It leads to the underfitting of the model.\n",
    "\n",
    "+ The training loss may steadily decrease with a negative slope\n",
    "+ The validation loss steadily decreases with a negative slope.\n",
    "\n",
    "It means the loss function has the opportunity to improve. \n",
    "The above graphs show that both the training loss and Val loss decrease dramatically in the beginning. They continuously decrease with negative slopes. \n",
    "\n",
    "They can be improved by increasing the number of epochs or using a different number of layers or neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "#Fix the seed\n",
    "tf.random.set_seed(1)\n",
    "#Increase the epochs to 10K\n",
    "history = model.fit(x=X_train,y=y_train,batch_size=64,epochs=10000,\n",
    "          validation_data=(X_test,y_test), verbose=0\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the train and validation loss to a df\n",
    "trainhist = pd.DataFrame(history.history)\n",
    "#Add the epoch index\n",
    "trainhist['epoch'] = history.epoch\n",
    "#Look at the latest performance\n",
    "trainhist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plot train loss\n",
    "sns.lineplot(x='epoch', y ='loss', data =trainhist)\n",
    "#Plot validation loss\n",
    "sns.lineplot(x='epoch', y ='val_loss', data =trainhist)\n",
    "#Add legends\n",
    "plt.legend(labels=['train_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models may overfit the model too. They have many weights to estimate. The model may not only fit the trend but also fit the noises.\n",
    "\n",
    "By looking at the train and validation loss above, we find that it is overfitting the data due to the following reasons.\n",
    "\n",
    "+ The training loss  steadily decrease with a negative slope\n",
    "+ The validation loss steadily increases with a positive slope.\n",
    "+ The differences between train loss and validation loss are huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout to present Neural Networks from Overfitting\n",
    "\n",
    "To address the overfitting in deep learning, Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov proposed a smart solution by using dropout.\n",
    "\n",
    "\"Deep neural nets with a large number of parameters are very powerful machine learning\n",
    "systems. However, overfitting is a serious problem in such networks. Large networks are also\n",
    "slow to use, making it difficult to deal with overfitting by combining the predictions of many\n",
    "different large neural nets at test time. Dropout is a technique for addressing this problem.\n",
    "The key idea is to randomly drop units (along with their connections) from the neural\n",
    "network during training. This prevents units from co-adapting too much. During training,\n",
    "dropout samples from an exponential number of different “thinned” networks. At test time,\n",
    "it is easy to approximate the effect of averaging the predictions of all these thinned networks\n",
    "by simply using a single unthinned network that has smaller weights. This significantly\n",
    "reduces overfitting and gives major improvements over other regularization methods. We\n",
    "show that dropout improves the performance of neural networks on supervised learning\n",
    "tasks in vision, speech recognition, document classification and computational biology,\n",
    "obtaining state-of-the-art results on many benchmark data sets.\"\n",
    "\n",
    "Source: Dropout: A Simple Way to Prevent Neural Networks from\n",
    "Overfitting (https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "\n",
    "The dropout can be summarized in the following graph taken from the paper above:\n",
    "\n",
    "<img src=\"dropout.jpg\">\n",
    "\n",
    "They also proposed the following practical guide for training dropout neural networks in their paper:\n",
    "+ **Network Size**: Due to the dropout rate of $p$, if there are $n$ neurons in a given layer, then only $np$ neurons  will keep in the networks after dropout. \"Therefore, if an n-sized layer is optimal for a standard neural net on any given task, a good dropout net should have at least $n/p$ units.\"\n",
    "+ **Learning Rate and Momentum**: We should use a high learning rate such as 10-100 times the learning rate for a standard neural net without dropout and/or momentum around 0.95 to 0.99 to significantly improve the performance and speed of learning.\n",
    "+ **Dropout Rate**: Typical dropout rates for hidden layers are in the range of 0.5 to 0.8. For input layers, the typical dropout rate is 0.8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add dropout and regularization on the weights in Tensorflow to address overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "model = keras.Sequential()\n",
    "# Input layer has 200 neurons\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "# Add dropout rate of 50%\n",
    "model.add(Dropout(0.5))\n",
    "# First hidden layer with 50 neurons\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "# Add dropout rate of 50%\n",
    "model.add(Dropout(0.5))\n",
    "# Second hidden layer with 50 neurons\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "# Add dropout rate of 50%\n",
    "model.add(Dropout(0.5))\n",
    "# Output layer has one and only one neuron\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "#Configure the model\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "\n",
    "#Fix the seed\n",
    "tf.random.set_seed(1)\n",
    "#Fit the Model\n",
    "history = model.fit(x=X_train,y=y_train,batch_size=64,epochs=10000,\n",
    "          validation_data=(X_test,y_test), verbose=0\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Convert the train and validation loss to a df\n",
    "trainhist = pd.DataFrame(history.history)\n",
    "#Add the epoch index\n",
    "trainhist['epoch'] = history.epoch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#Plot train loss\n",
    "sns.lineplot(x='epoch', y ='loss', data =trainhist)\n",
    "#Plot validation loss\n",
    "sns.lineplot(x='epoch', y ='val_loss', data =trainhist)\n",
    "#Add legends\n",
    "plt.legend(labels=['train_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the dropout helps the convergence of the algorithms. Both the train losss and validation loss decay exponetially in the beginning and almost become constant in the end.\n",
    "\n",
    "Althoug the performance improves, there still have some opportunities to improve. We will learn how to improve it using Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning in TensorFlow\n",
    "\n",
    "\n",
    "There are two types of parameters in  deep learning. (see: https://www.tensorflow.org/tutorials/keras/keras_tuner.)\n",
    "+ **Model Parameters** that can be estimated from the given data by finding the optimal values. For example, the weights and bias are model parameters in neuron networks\n",
    "+ **Hyperparamters** that must be specified by data scientists  before training the models.  They are typically two types of hyperparameters in deep learning:\n",
    "    + **Model Hyperparameters**: for example, the number of hidden layers and the number of neurons in each layer.\n",
    "    + **Algorithm Hyperparameters**: for example, the learning rate in different optimizers.\n",
    "\n",
    "Let's look at how to tune the hyperparameters in TensorFlow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Keras Tuner\n",
    "\n",
    "We first check whether the keras-tuner was installed or not. If we did not install it, then we should install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#Check the existence of the keras-tuner library\n",
    "if ( 'keras-tuner' not in sys.modules):\n",
    "    #If it was not installed, then install it\n",
    "    !pip install -q -U keras-tuner\n",
    "#Import the library of keras-tuner\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model by Specifying Hyperameter Range\n",
    "\n",
    "We need to define our model by specifying the hyperparameters. For illustration purposes, we use the same model as before. We specify the following hyperameters:\n",
    "\n",
    "+ Search the number of neurons from 50-500 with a stepsize of 50 in the first input layer.\n",
    "+ Search the dropout rate in the first input layer in the range of 0.2-0.8 with a stepsize of 0.1.\n",
    "+ Search the lerning rate from 0.01, 0.001, or 0.0001.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    " \n",
    "  # Tune the number of units in the first input layer\n",
    "  # Search the number of neurons from 50-500 with a stepsize of 50 in the first input layer.\n",
    "  hp_units1 = hp.Int('units', min_value = 50, max_value = 500, step = 50)\n",
    "  model.add(layers.Dense(units = hp_units1, activation = 'relu'))\n",
    "  # Tune the dropout rate in the first input layer\n",
    "  # Search the dropout rate in the first input layer in the range of 0.2-0.8 with a stepsize of 0.1.\n",
    "  hp_dropout1 = hp.Float('rate', min_value = 0.2, max_value = 0.8, step = 0.1)\n",
    "  model.add(Dropout(rate = hp_dropout1))\n",
    "  # first hidden layer with 100 neurons\n",
    "  model.add(layers.Dense(100, activation='relu'))\n",
    "  # add dropout rate of 50%\n",
    "  model.add(Dropout(0.5))\n",
    "  # second hidden layer with 50 neurons\n",
    "  model.add(layers.Dense(50, activation='relu'))\n",
    "  # add dropout rate of 50%\n",
    "  model.add(Dropout(0.5))\n",
    "  # output layer has one and only one neuron\n",
    "  model.add(layers.Dense(1))\n",
    "  \n",
    " # Tune the learning rate for the optimizer \n",
    " # Search the lerning rate from 0.01, 0.001, or 0.0001.\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "\n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = 'mse', \n",
    "                metrics = [tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Tuner and Perform Hypertuning\n",
    "\n",
    "To tune the models, we need to instantiate the model first. We can choose the following four tuners in keras:\n",
    "+ **BayesianOptimization** (see https://keras-team.github.io/keras-tuner/documentation/tuners/#bayesianoptimization-class)\n",
    "+ **Hyperband** (see https://keras-team.github.io/keras-tuner/documentation/tuners/#hyperband-class)\n",
    "+ **RandomSearch** (see https://keras-team.github.io/keras-tuner/documentation/tuners/#randomsearch-class)\n",
    "+ **Sklearn**   (see https://keras-team.github.io/keras-tuner/documentation/tuners/#sklearn-class)\n",
    "\n",
    "For illustration purposes, we try Hyperband tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder, #Specify the model\n",
    "                     objective = 'val_loss', #Specify the objective funciton\n",
    "                     max_epochs = 100, #Specify the maximum epochs\n",
    "                     directory = 'my_dir', #Specify the file path\n",
    "                     project_name = 'tuningRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to be overwhelmed by all the outputs of the tuners. It is a good practice to define a callback to clear the training outputs at the end of every training step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "#Clear all the training outputs\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform the search on the defined hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the search on the defined hyperparameter space by specifying the callback to clear the training outputs\n",
    "tuner.search(X_train, y_train, epochs = 100, validation_data = (X_test,y_test), callbacks = [ClearTrainingOutput()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print out the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "#Use f-strings to format the outputs\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the input layer =  {best_hps.get('units')}. \n",
    "The optimal droupout rate in the input layer = {best_hps.get('rate')}\n",
    "The optimal learning rate for the optimizer of Adam = {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the Model with the Optimal Hyperparameters\n",
    "\n",
    "After we get the optimal hyperparameters, we need to retrain the model using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(X_train, y_train, epochs = 100, validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimal model may be improved again by tuning other hidden layers. You may try it by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Models in TensorFlow\n",
    " \n",
    "It is very time-consuming to train the model. It may take a couple of days to train and tune the hyperparameters using a laptop and PC. Once a model is trained, the data scientists/researchers like to share the model with codes and optimal weights. Therefore other people don't need to retrain the model. They can load the optimal weights into memory and then forecast the new data, significantly reducing the running time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install h5py and pyyaml Packages\n",
    "\n",
    "To save the model weights, we need to install two require packages:\n",
    "+ pyyaml: YAML is a data serialization format designed for human readability and interaction with scripting languages. PyYAML is a YAML parser and emitter for Python. To install this package with conda run:\n",
    "**conda install -c anaconda pyyaml**\n",
    "+ h5py: The h5py package provides both a high- and low-level interface to the HDF5 library from Python. The HDF5 can help store and organize large amounts of data very efficiently. To install this package with conda run: \n",
    "**conda install -c anaconda h5py**\n",
    "\n",
    "Let's create a model with the optimal hyperparameters with the same network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_create():\n",
    "  model = keras.Sequential()\n",
    "  #Set the optimal units of 450 found by tuner\n",
    "  model.add(layers.Dense(units = 450, activation = 'relu'))\n",
    "  #Set he optimal dropout rate of 0.2 found by tuner\n",
    "  model.add(Dropout(rate = 0.2))\n",
    "  #First hidden layer with 100 neurons\n",
    "  model.add(layers.Dense(100, activation='relu'))\n",
    "  #Add dropout rate of 50%\n",
    "  model.add(Dropout(0.5))\n",
    "  #Second hidden layer with 50 neurons\n",
    "  model.add(layers.Dense(50, activation='relu'))\n",
    "  #Add dropout rate of 50%\n",
    "  model.add(Dropout(0.5))\n",
    "  #Output layer has one and only one neuron\n",
    "  model.add(layers.Dense(1))\n",
    "  #Set the optimal learning rate of 0.01 found by tuner\n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.01),\n",
    "                loss = 'mse', \n",
    "                metrics = [tf.keras.metrics.MeanSquaredError()])\n",
    "  return model\n",
    "# Create the model\n",
    "model = model_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Checkpoints During Training\n",
    "\n",
    "\n",
    "We want to save the model weights; then we can reuse the model instead of retraining the model. To save the model weights during the training, we need to create a **tf.keras.callbacks.ModelCheckpoint callback** (see https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    ") that saves weights only during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory to save the weights\n",
    "import os\n",
    "cp_path = \"training/cp.regr\"\n",
    "cp_dir = os.path.dirname(cp_path)\n",
    "\n",
    "# Create a callback to save the model's weights\n",
    "# We only save the best weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=cp_path, save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model and specif the defined callback using callbacks=[]\n",
    "model.fit(X_train, y_train, epochs = 100, validation_data = (X_test,y_test),\n",
    "         callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Weights and Evaluate the Model \n",
    "\n",
    "Suppose you save the model weights only; then, you can evaluate the model on the new dataset without training this model.\n",
    "But you still need to specify the model architecture. The model architecture must be the same as that of the model trained. Then you can load the weights in the memory and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, create the model\n",
    "model = model_create()\n",
    "#Second, load the weights\n",
    "model.load_weights(cp_path)\n",
    "#Note here, we don't train the model at all\n",
    "#Third evaluate the model on the new dataset\n",
    "loss = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print(\"Reloaded model from file with loss: {:5.2f}\".format(loss[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model on the Test Data\n",
    "\n",
    "Finally, let's evaluate the model. Since it is a regression problem, we can look at the following metrics:\n",
    "\n",
    "+ Mean squared error\n",
    "+ Root of mean squared error\n",
    "+ Mean absolute error\n",
    "\n",
    "We  may use other metrics. Please consult the scikit-learn document. (see https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'The mean square error is {0:.4f}'.format(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'The root of mean square error is {0:.4f}'.format(mean_squared_error(y_test,y_pred,squared = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'The mean absolute error is {0:.4f}'.format(mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
