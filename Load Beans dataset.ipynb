{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8rgWizHftvu"
   },
   "source": [
    "## 1.Load the beans dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ycjF6Z_Wftvv"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from collections import Counter\n",
    "# Enable eager execution\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZ_dn7WUftvw"
   },
   "source": [
    "## load the dataset\n",
    "\n",
    "Please install tensorflow_datasets packages using\n",
    "\n",
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall tensorflow-datasets\n",
    "pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tfds-nightly as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tfds-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XprmJ5CGftvw",
    "outputId": "11d9b957-500a-495b-cd3b-e3e42ced5df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\whitl\\tensorflow_datasets\\beans\\0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389e54615fce40bcb3f959551e460b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db63f4756c74ddc85052cc38ced211d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\whitl\\tensorflow_datasets\\beans\\0.1.0.incompleteW51M6S\\beans-train.tfrecord*...:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\whitl\\tensorflow_datasets\\beans\\0.1.0.incompleteW51M6S\\beans-validation.tfrecord*...:   0%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\whitl\\tensorflow_datasets\\beans\\0.1.0.incompleteW51M6S\\beans-test.tfrecord*...:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset beans downloaded and prepared to C:\\Users\\whitl\\tensorflow_datasets\\beans\\0.1.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "tfds.core.DatasetInfo(\n",
      "    name='beans',\n",
      "    full_name='beans/0.1.0',\n",
      "    description=\"\"\"\n",
      "    Beans is a dataset of images of beans taken in the field using smartphone\n",
      "    cameras. It consists of 3 classes: 2 disease classes and the healthy class.\n",
      "    Diseases depicted include Angular Leaf Spot and Bean Rust. Data was annotated by\n",
      "    experts from the National Crops Resources Research Institute (NaCRRI) in Uganda\n",
      "    and collected by the Makerere AI research lab.\n",
      "    \"\"\",\n",
      "    homepage='https://github.com/AI-Lab-Makerere/ibean/',\n",
      "    data_dir='C:\\\\Users\\\\whitl\\\\tensorflow_datasets\\\\beans\\\\0.1.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=171.69 MiB,\n",
      "    dataset_size=171.63 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(500, 500, 3), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=3),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=128, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=1034, num_shards=2>,\n",
      "        'validation': <SplitInfo num_examples=133, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@ONLINE {beansdata,\n",
      "        author=\"Makerere AI Lab\",\n",
      "        title=\"Bean disease dataset\",\n",
      "        month=\"January\",\n",
      "        year=\"2020\",\n",
      "        url=\"https://github.com/AI-Lab-Makerere/ibean/\"\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(bn_train, bn_validation, bn_test),bn_info = tfds.load(\n",
    "\n",
    "    name = 'beans',\n",
    "\n",
    "    split = ['train', 'validation', 'test'],\n",
    "\n",
    "    as_supervised = True,\n",
    "\n",
    "    with_info = True)\n",
    "\n",
    "print(bn_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcvogv3Oftvx",
    "outputId": "e5f6e2ff-9a7b-44ac-8b75-b7c71d03f6df"
   },
   "outputs": [],
   "source": [
    "# the training dataset has features/image and label/target\n",
    "for image, label in bn_train.take(1):  # example is (image, label)\n",
    "  print(image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWNUyehQftvy",
    "outputId": "b339a340-09ad-447a-d240-e8e38c6284e6"
   },
   "outputs": [],
   "source": [
    "print(bn_info.features[\"label\"].num_classes)\n",
    "print(bn_info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8OQIm8Hftvy",
    "outputId": "c4e99b7f-50d5-4592-fcfe-96fdc20147b6"
   },
   "outputs": [],
   "source": [
    "print(bn_info.features.shape)\n",
    "print(bn_info.features.dtype)\n",
    "print(bn_info.features['image'].shape)\n",
    "print(bn_info.features['image'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg9ayWJ0ftvz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Image parameters:\n",
    "image_height = 500\n",
    "image_width = 500\n",
    "num_channels = 3 # RGB\n",
    "num_classes = 3 # healthy, angular leaf spot disease, bean rust disease\n",
    "\n",
    "# Pipeline hyperparameters:\n",
    "batch_size = 32\n",
    "\n",
    "# Let's keep the dimensions the same (no resizing for now)\n",
    "# the inputs are the image, label and target size\n",
    "def normalize_image(image, label, target_height = 500, target_width = 500):\n",
    "    \"\"\"Normalizes images: `unit8` -> `float32` and resizes images\n",
    "    by keeping the aspect ratio the same without distortion.\"\"\"\n",
    "    image = tf.cast(image, tf.float32)/255.\n",
    "    image = tf.image.resize_with_crop_or_pad(image, target_height, target_width)\n",
    "    return image, label\n",
    "#apply normalize_image() function to each of image avoid writing a loop\n",
    "bn_train = bn_train.map(normalize_image, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "bn_train = bn_train.cache().shuffle(1000)\n",
    "bn_train = bn_train.shuffle(bn_info.splits['train'].num_examples)\n",
    "bn_train = bn_train.batch(batch_size)\n",
    "bn_train = bn_train.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the similar tasks for the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6WBm_uSftvz"
   },
   "outputs": [],
   "source": [
    "bn_validation = bn_validation.map(\n",
    "    normalize_image, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "bn_validation = bn_validation.batch(batch_size)\n",
    "bn_validation = bn_validation.cache()\n",
    "bn_validation = bn_validation.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "bn_test = bn_test.map(\n",
    "    normalize_image, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "bn_test = bn_test.batch(batch_size)\n",
    "bn_test = bn_test.cache()\n",
    "bn_test = bn_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_vIWQZtftv0",
    "outputId": "bab62099-f126-48e9-9402-f00ffc019a76"
   },
   "outputs": [],
   "source": [
    "def return_class_labels(ds):\n",
    "    \"\"\"\"Returns a list of class labels from a `DatasetV1Adapter` object.\"\"\"\n",
    "    l_labels = []\n",
    "    # since we handle labels only in this funciton, we discard the first parameter that is image\n",
    "    # we take out one batch that is 32 images and lables\n",
    "    for _, labels in ds.take(-1):\n",
    "        labels = labels.numpy()\n",
    "        l_labels.append(labels[:])\n",
    "    # the l_labels is a nest of list , for example [ [1, 0, 2, ...], [2,1, 1,..], ..]\n",
    "    # return a single list by using list comprehension\n",
    "    #[1, 0, 2, ..., 2, 1, 1, ..]\n",
    "    return [item for sublist in l_labels for item in sublist]\n",
    "\n",
    "training_labels = return_class_labels(bn_train)\n",
    "print(\"The distribution of training labels is: \", (Counter(training_labels)))\n",
    "\n",
    "validation_labels = return_class_labels(bn_validation)\n",
    "print(\"The distribution of validation labels is: \", (Counter(validation_labels)))\n",
    "\n",
    "test_labels = return_class_labels(bn_test)\n",
    "print(\"The distribution of test labels is: \", (Counter(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 936
    },
    "id": "Hd9awgDEftv0",
    "outputId": "b4786af7-134a-4a4f-8c5d-724bd3df4739"
   },
   "outputs": [],
   "source": [
    "# take out one batch that is 32 images\n",
    "example = bn_train.take(1)\n",
    "# each batch has two componets, image and label\n",
    "# sample=(image, lable)\n",
    "for sample in example:\n",
    "    image, label = sample[0], sample[1]\n",
    "    #convert them to np\n",
    "    image = image.numpy()\n",
    "    label = label.numpy()\n",
    "\n",
    "n_cols, n_rows = 4, 4\n",
    "plt.rcParams['figure.figsize'] = [n_cols*4, n_rows*4]\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(1, n_cols*n_rows + 1):\n",
    "    ax = fig.add_subplot(n_rows, n_cols,i)\n",
    "    # we label the subplots using names instead of magic numbers such as 0, 1 and 2\n",
    "    ax.text(5, -9, \"Angular Leaf Spot\" if label[i] == 0 else \"Bean Rust\" if label[i] == 1 else \"Healthy\",\n",
    "            color = 'red', fontsize = 20)\n",
    "    ax.imshow(image[i, :, :, :], cmap = plt.get_cmap(\"jet\"))  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "r4Tr8WMIftv9",
    "y5p1lWoRftwB",
    "GafZxUsYftwB",
    "OJdXgGSxftwC"
   ],
   "name": "Project04_Ramireddy_Manoharareddy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
